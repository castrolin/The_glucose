{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import xlrd\n",
    "# the different initial condition is different input element\n",
    "# the input element need to \"Normalization\"\n",
    "# Taking off the label and then the output will become a single node\n",
    "# the furture work is \n",
    "\"\"\"\n",
    "1. Normalization the input (OKAY)\n",
    "2. Set the initial condition to one element input (OKAY)\n",
    "3. Reducing the input conditio to robust the model!! (OKAY)\n",
    "4. the hiddent layer only need one layer in the layer!!(OKay)\n",
    "5. (4 important input+ 1 initialcondition input) one output\n",
    "\"\"\"\n",
    "#the post-process\n",
    "#Input the Excel becomne a input\n",
    "data = xlrd.open_workbook('traindatabase.xlsx')\n",
    "database= []\n",
    "for n in range(len(data.sheet_names())): #len(data.sheet_names()) is the number of worksheet \n",
    "    print('worksheet {} :'.format(n),end='')\n",
    "    table = data.sheets()[n]\n",
    "    for i in range(table.nrows):\n",
    "        if i != 0: # neglect the first row!!\n",
    "            #print(table.row_values(i))\n",
    "            database.append(table.row_values(i))\n",
    "print('the number of data = ',len(database)) # the data length\n",
    "print('the element of each data is',len(database[0])) # How many input\n",
    "#the data is list so the format is database[row][col]\n",
    "# Split the database to np.array formate(train element,the number of data)\n",
    "trainX = np.zeros((len(database[0])-1,len(database)))\n",
    "trainY = np.zeros((1,len(database)))\n",
    "for i in range(len(database)):\n",
    "    n=0\n",
    "    for j in range(1,len(database[0])):\n",
    "        trainX[n,i]=database[i][j]\n",
    "        n=n+1\n",
    "    trainY[0,i] =database[i][0]\n",
    "# Making the label of the number 100 200 300 400 500\n",
    "#for i in range(len(database)):\n",
    "    #if database[i][0] == 100:\n",
    "        #train_y[:,i]=np.transpose([1,0,0,0,0])\n",
    "    #if database[i][0] == 200:\n",
    "        #train_y[:,i]=np.transpose([0,1,0,0,0])\n",
    "    #if database[i][0] == 300:\n",
    "        #train_y[:,i]=np.transpose([0,0,1,0,0])\n",
    "    #if database[i][0] == 400:\n",
    "        #train_y[:,i]=np.transpose([0,0,0,1,0])\n",
    "    #if database[i][0] == 500:\n",
    "        #train_y[:,i]=np.transpose([0,0,0,0,1])\n",
    "# Changing the formate to suit the traning set [[output 1 element],[input 16 element]]\n",
    "#train_data = [[[],[]]]*len(database)\n",
    "#for i in range(0,len(database)):\n",
    "    #train_data[i][0]=[database[i][0]]\n",
    "    #train_data[i][1]=database[i][1:len(database[0])]\n",
    "#train_data[0][1].append(database[2][1:len(database[0])])\n",
    "#print(train_data)\n",
    "print(trainX[:,36])\n",
    "#print(train_y)\n",
    "#print(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization \n",
    "# train_x_r train_y_r\n",
    "train_x_r = np.zeros((len(database[0])-1,len(database)))\n",
    "train_y_r = np.zeros((1,len(database)))\n",
    "# Normalize the input (7 element)\n",
    "for i in range(len(database[0])-1):\n",
    "    if i !=0:  # negelect the output\n",
    "        mintrain = min(trainX[i,:])\n",
    "        maxtrain = max(trainX[i,:])\n",
    "        for j in range(len(database)):\n",
    "            train_x_r[i,j] = (trainX[i,j]-mintrain)/(maxtrain-mintrain)\n",
    "# Normalize the output\n",
    "mintrainY = min(trainY[0,:])\n",
    "maxtrainY = max(trainY[0,:])\n",
    "for i in range(len(database)):\n",
    "    train_y_r[0,i] = (trainY[0,i]-mintrainY)/(maxtrainY-mintrainY)\n",
    "    \n",
    "print(trainX[:,0])\n",
    "print(train_x_r[:,0])\n",
    "print(trainY[:,0])\n",
    "print(train_y_r[:,0])\n",
    "print(min(trainY[0,:]),'\\t',max(trainY[0,:]))\n",
    "#for i in range(len(database[0])-1):\n",
    "    #print(min(trainX[i,:]),'\\t',max(trainX[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_x[:,36])\n",
    "# split the training data(80%) and testing data(20%)\n",
    "import random\n",
    "res = random.sample(range(1, len(database)), int(len(database)*0.8))\n",
    "train_x = np.zeros(((len(database[0])-1),int(len(database)*0.8)))\n",
    "train_y = np.zeros((1,int(len(database)*0.8)))\n",
    "#print(train_x.shape[0])\n",
    "\n",
    "n=0\n",
    "for i in res:\n",
    "    train_x[:,n]=train_x_r[:,i]\n",
    "    train_y[0,n]=train_y_r[0,i]\n",
    "    n=n+1\n",
    "#create the matrix to find 20% number of data\n",
    "test_index=[]\n",
    "for j in range(0,len(database)):\n",
    "    n=0\n",
    "    for k in res:\n",
    "        if j-k !=0:\n",
    "            n=n+1\n",
    "    if n != len(res)-1:\n",
    "        test_index.append(j)\n",
    "\n",
    "test_x = np.zeros(((len(database[0])-1),len(test_index)))\n",
    "test_y = np.zeros((1,len(test_index)))\n",
    "\n",
    "n=0\n",
    "for i in test_index:\n",
    "    test_x[:,n] = train_x_r[:,i]\n",
    "    test_y[0,n] = train_y_r[0,i]\n",
    "    n=n+1\n",
    "    \n",
    "print(res)\n",
    "print(train_x[:,10])\n",
    "print(test_x[:,10])\n",
    "#print(len(res))\n",
    "#print(len(test_index))\n",
    "print(\".......................................\")\n",
    "print(\".......................................\")\n",
    "#print(test_x[:,20])\n",
    "print(test_index)\n",
    "print(train_y[:,10])\n",
    "print(test_y[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_x[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BPNN structure\n",
    "# The initialize parameters\n",
    "def NL_initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "    \n",
    "                    W1 -- weight matrix of shape (Layer_dims[W1], Layer_dims[W0])\n",
    "                    b1 -- bias vector of shape (Layer_dims[W1], 1)\n",
    "                    W2 -- (Layer_dims[W2], Layer_dims[W1])\n",
    "                    b1 -- (Layer_dims[W2], 1)\n",
    "                    \n",
    "                    So we can get these rule: (\"L\" denotes the number of layer.)\n",
    "                    Shape of \"WL\" -- (Layer_dims[L], Layer_dims[L-1])\n",
    "                    Shape of \"bL\" -- (Layer_dims[L], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    Layers = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, Layers):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters\n",
    "# The Forward pass\n",
    "def NL_forwardpass(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  \n",
    "    linear_cache = {}\n",
    "    activation_cache = {}\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        linear_cache = {}\n",
    "        activation_cache = {}\n",
    "        A_prev = A\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        \n",
    "        # Linear calculation\n",
    "        linear_cache[\"A_prev\" + str(l)] = A_prev\n",
    "        linear_cache[\"W\" + str(l)] = W\n",
    "        linear_cache[\"b\" + str(l)] = b\n",
    "        Z = W.dot(A_prev) + b\n",
    "        \n",
    "        # ReLU calculation: ReLU\n",
    "        A = np.maximum(0, Z)\n",
    "        activation_cache[\"Z\" + str(l)] = Z\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    ## The final layer.\n",
    "    linear_cache = {}\n",
    "    activation_cache = {}\n",
    "    W = parameters['W' + str(L)]\n",
    "    b = parameters['b' + str(L)]\n",
    "    \n",
    "    # Linear calculation\n",
    "    # \n",
    "    linear_cache[\"A_prev\" + str(L)] = A\n",
    "    linear_cache[\"W\" + str(L)] = W\n",
    "    linear_cache[\"b\" + str(L)] = b\n",
    "    ZL = W.dot(A) + b\n",
    "    \n",
    "    # Activative calculation: Sigmoid\n",
    "    AL = 1/(1+np.exp(-ZL))\n",
    "    activation_cache[\"Z\" + str(L)] = ZL\n",
    "    \n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL, caches\n",
    "# Calculate the cost, In other words, calculate the error function\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]   # number of class\n",
    "    # Logistic regression\n",
    "    #cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    #cost = np.squeeze(cost)  # makes sure cost is the dimension we expect.\n",
    "    #return cost\n",
    "    return 0.5*np.sum((AL-Y)**2)\n",
    "# The backward pass \n",
    "def NL_backwardpass(AL, X, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    ## Initializing the backpropagation\n",
    "    ## Computes the gradient of AL. (AL means the y-hat of model.)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    ## L-th layer gradients.\n",
    "    ## (Sigmoid -> Linear)\n",
    "    current_cache = caches[L-1]   # The index of caches is in the range of 0 to L-1.\n",
    "    linear_cache, activation_cache = current_cache\n",
    "    \n",
    "    # dZL (Sigmoid backward)\n",
    "    s = 1/(1+np.exp( -activation_cache[\"Z\" + str(L)] ))\n",
    "    dZL = dAL * s * (1-s)\n",
    "    \n",
    "    # dA_prev (Linear backward)\n",
    "    A_prev = linear_cache[\"A_prev\" + str(L)]\n",
    "    W = linear_cache[\"W\" + str(L)]\n",
    "    b = linear_cache[\"b\" + str(L)]\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZL, A_prev.T)\n",
    "    db = 1./m * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZL)\n",
    "\n",
    "    ## Save grads.(The L-th layer)\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = dA_prev, dW, db\n",
    "    \n",
    "    ## The value of l is decreased from L-1 to 1.\n",
    "    for l in reversed(range(1, L)):\n",
    "        ## l-th layer gradients.\n",
    "        ## (ReLU -> Linear), Example: caches[2] contains: A_prev3, W3, b3\n",
    "        current_cache = caches[l-1]\n",
    "        linear_cache, activation_cache = current_cache\n",
    "    \n",
    "        # dZ (ReLU backward)\n",
    "        Z = activation_cache[\"Z\" + str(l)]\n",
    "        dZ = np.array(grads[\"dA\" + str(l)], copy=True)\n",
    "        dZ[Z<=0] = 0\n",
    "        assert (dZ.shape == Z.shape)  # check shape\n",
    "        \n",
    "        # dA (Linear backward)\n",
    "        A_prev = linear_cache[\"A_prev\" + str(l)]\n",
    "        W = linear_cache[\"W\" + str(l)]\n",
    "        b = linear_cache[\"b\" + str(l)]\n",
    "        m = A_prev.shape[1]\n",
    "        dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "        db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA_prev = np.dot(W.T,dZ)\n",
    "        assert (dA_prev.shape == A_prev.shape)  # check shape\n",
    "        assert (dW.shape == W.shape)\n",
    "        assert (db.shape == b.shape)\n",
    "        \n",
    "        ## Save grads.(the l-th layer)\n",
    "        grads[\"dA\" + str(l-1)], grads[\"dW\" + str(l)], grads[\"db\" + str(l)] = dA_prev, dW, db\n",
    "    \n",
    "    return grads\n",
    "# Update parameters\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    \n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function\n",
    "\"\"\"initializing the parameter -> forward pass -> compute cost -> backward pass -> update parameter\n",
    "    repeat the aformation step until the end of iteration\"\"\"\n",
    "def NL_nn_model(X, Y, layers_dims, num_iterations = 5000, learning_rate=0.08, print_cost = False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    layers_dims -- size of all the hidden layers\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # Initialize W, b\n",
    "    parameters = NL_initialize_parameters(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward pass\n",
    "        AL, caches = NL_forwardpass(X, parameters)\n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward pass\n",
    "        grads = NL_backwardpass(AL, X, Y, caches)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost so far every 500 iterations\n",
    "        if i % 500 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "                #print(AL.shape)\n",
    "    #print(AL)\n",
    "    # Append the cost of final layer into \"costs\".\n",
    "    costs.append(cost)\n",
    "    plt.figure(num=1, figsize=(8,5))\n",
    "    plt.semilogy(costs)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Learning Rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "\"\"\"\n",
    "    the def function cannot return AL and parameter because the parameters will change formation from\n",
    "    dictionary to index when return them at the same time\n",
    "\"\"\"\n",
    "# Predict\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((5,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = NL_forwardpass(X, parameters)  \n",
    "    \"\"\"\n",
    "    # probas is the output of NN and its shape is (5,95)\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.25:\n",
    "            p[0,i] = 100\n",
    "        if probas[1,i] >0.25:\n",
    "            p[1,i] = 200\n",
    "        if probas[2,i] > 0.25:\n",
    "            p[2,i] = 300\n",
    "        if probas[3,i] >0.25:\n",
    "            p[3,i] = 400\n",
    "        if probas[4,i] > 0.25:\n",
    "            p[4,i] = 500\n",
    "    \"\"\"    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    #print(\"Accuracy: \"  + str(1-(0.5*np.sum((probas-y)**2))))\n",
    "    print(\"Accuracy: \"+str(abs(probas-y)/probas))\n",
    "        \n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the procedure of traning\n",
    "#train_x = train_data[1]\n",
    "#train_y = train_data[0]\n",
    "layers_dims = [len(database[0])-1,10,3,1] # 4-Layer model\n",
    "parameters = NL_nn_model(train_x, train_y, layers_dims, num_iterations=60000000,learning_rate=0.00005, print_cost=True)\n",
    "# calculate the accuracy of trainning\n",
    "#print(\"Training accuracy:\")\n",
    "#predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy:\")\n",
    "predictions_train = predict(train_x, train_y, parameters)\n",
    "#print(train_x[:,1].shape[1])\n",
    "#print(train_y.shape[1])\n",
    "#print(train_x[:,1])\n",
    "#print(train_x[:,1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_index =14\n",
    "test = np.zeros((len(database[0])-1,1))\n",
    "test[:,0] = test_x[:,choose_index]\n",
    "answear = test_y[:,choose_index]\n",
    "kk = predict(test, test_y[:,choose_index], parameters)\n",
    "print(\"test index= \"+str(test_index[choose_index]),'\\t',\"it can go back to check the database index\")\n",
    "print(\"Testing input:\"+ str(test),\"\\t\",\"it is a normalize input\")\n",
    "print(\"//////////////////the real input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "print(\"the original data = \",trainX[:,test_index[choose_index]],'\\t',trainY[:,test_index[choose_index]])\n",
    "print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<--------------->>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "print(\"Prediction: \"+str(kk),\"The Answear:\"+str(answear),\"\\t\",\"it is normalize answear\")\n",
    "print(\"//////////////////the real answear\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "print(\"Prediction: \"+str(500*kk),\"The Answear:\"+str(500*answear),\"\\t\",\"it is normalize answear\")\n",
    "\n",
    "#print(test_y[:,1choose_index])\n",
    "#print(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "print(\"test index= \",test_index[15],'\\t')\n",
    "print(\"the original data = \",trainX[:,test_index[15]],'\\t',trainY[:,test_index[15]])\n",
    "#print(\"the normalize data = \",trainX[:,test_index[10]],'\\t',trainY[:,test_index[10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdfdict as hdf\n",
    "# save the \"parameters\" due to \"parameters\" is a dictionary\n",
    "fname  = 'parameters.h5'\n",
    "hdf.dump(parameters,fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = hdf.load('parameters.h5')\n",
    "s =dict(para) # make it open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
